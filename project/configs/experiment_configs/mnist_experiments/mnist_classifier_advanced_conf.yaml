# to execute this experiment run:
# python train.py --exp_conf configs/experiment_configs/mnist_experiments/mnist_classifier_advanced_conf.yaml

# (required section)
trainer_args:
    min_epochs: 1               # train for at least this many epochs (denies early stopping)
    max_epochs: 10
    gradient_clip_val: 0.5      # gradient clipping (helps with exploding gradient issues)
    accumulate_grad_batches: 2  # perform optimization step after accumulating gradient from 2 batches
    fast_dev_run: False         # execute 1 training, 1 validation and 1 test epoch only
    limit_train_batches: 0.8    # train only on 80% of training data
    limit_val_batches: 1.0      # validate on 100% of validation data
    limit_test_batches: 1.0     # test on 100% of test data
    val_check_interval: 0.5     # perform validation twice per epoch

# (required section)
model:
    load_from:
        model_path: "pytorch_modules/lightning_models/simple_mnist_classifier.py"
        model_class: "LitModel"
    hparams:  # you can add here any parameters you want and then read them in model
        lr: 0.001
        weight_decay: 0.00001
        input_size: 784  # img size is 1*28*28
        output_size: 10  # there are 10 digit classes
        lin1_size: 256
        lin2_size: 256
        lin3_size: 128

# (required section)
datamodule:
    load_from:
        datamodule_path: "lightning_datamodules/mnist_datamodule/mnist_datamodule.py"
        datamodule_class: "MNISTDataModule"
    hparams:  # you can add here any parameters you want and then read them in datamodule
        batch_size: 256
        train_val_test_split: [55_000, 5_000, 10_000]
        num_workers: 1              # num of processes used for loading data in parallel
        pin_memory: False           # dataloaders will copy tensors into CUDA pinned memory before returning them

# (optional section)
# pytorch seed for this experiment
# it also affects random_split() method used in mnist_datamodule.py
# overwrites default pytorch seed set in project_config.yaml
pytorch_seed:
    use_seed: True
    pytorch_seed: 1234

# (optional section)
# Weights&Biases logger args
# this will be combained with default wandb args set in project_config.yaml (overwrites repeating args)
wandb_args:
    group: ""
    job_type: "train"
    tags: ["v2", "uwu"]
    notes: "This is example run."

# (optional section)
# path to .yaml file containing configurations of all callbacks which will be used for this experiment
# this will be combained with default callbacks set in project_config.yaml (overwrites repeating callbacks)
callbacks_configs_path: "configs/callbacks_configs/mnist_callabcks.yaml"

# (optional section)
# use this if you want to resume training from checkpoint
resume_training:
    checkpoint_path: None  # path to your checkpoint
    wandb_run_id: None  # you can add id of wandb run you want to resume but it's optional
