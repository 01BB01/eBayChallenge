# @package _global_

# to execute this experiment run:
# python train.py +experiment=exp_example_with_paths

defaults:
    - override /trainer: null                           # override trainer to null so it's not loaded from main config defaults
    - override /model: null                             # override model to null so it's not loaded from main config defaults
    - override /datamodule: null                        # override datamodel to null so it's not loaded from main config defaults
    - override /seeds: null                             # override seeds to null so it's not loaded from main config defaults
    - override /callbacks: default_callbacks.yaml       # choose callback set from 'configs/callbacks/' folder
    - override /logger: null                            # choose logger from 'configs/logger/' folder or set it from console when running experiment:
                                                        # `python train.py +experiment=exp_example_with_paths logger=wandb`

# we override default configurations with nulls to prevent them from loading at all - instead we define all modules
# and their paths directly in this config so everything is stored in one place and we have more readibility

seeds:
    pytorch_seed: 12345

trainer:
    _target_: pytorch_lightning.Trainer
    min_epochs: 1
    max_epochs: 10
    gradient_clip_val: 0.5
    weights_summary: null
    gpus: 0

model:
    _target_: src.models.mnist_model.LitModelMNIST
    optimizer: adam
    lr: 0.001
    weight_decay: 0.000001
    architecture: SimpleDenseNet
    input_size: 784
    lin1_size: 256
    dropout1: 0.30
    lin2_size: 256
    dropout2: 0.25
    lin3_size: 128
    dropout3: 0.20
    output_size: 10

datamodule:
    _target_: src.datamodules.mnist_datamodule.MNISTDataModule
    data_dir: ${data_dir}
    batch_size: 64
    train_val_test_split: [55_000, 5_000, 10_000]
    num_workers: 1
    pin_memory: False
