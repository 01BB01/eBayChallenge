# default args passed to Lightning Trainer on every run/experiment
# if you specify the same args in experiment config then they will overwrite the ones set here
default_trainer_args:
    num_of_gpus: 0                 # set -1 to train on all GPUs abailable, set 0 to train on CPU only
    progress_bar_refresh_rate: 10   # refresh terminal every 10 batches
    weights_summary: "top"          # print model summary (alternatively "full")


# default pytorch seed used on every run/experiment
# this can be overwritten by
default_pytorch_seed:
    use_seed: True
    pytorch_seed: 1234


# default loggers initialised on every run/experiment
# if you specify this section in experiment config then it will be totally overwritten
default_loggers:
    wandb:  # Weights&Biases config (just remove this section if you don't want tu ever use Weights&Biases)
        use_wandb: True
        log_gradients: False    # set True to log gradient histograms (avoid this for very big lightning_models)
        log_optimizer_name: True
        log_train_val_test_data_lengths: True
        default_args:
            project: "project_template_test"
            entity: "kino"          # change to your username or name of your team on wandb
            log_model: True         # set True to automatically upload model checkpoint to wandb cloud after training
            offline: False          # set True to store all logs only locally
    tensorboard:
        use_tensorboard: False
        default_args: {}


# default lightning_callbacks initialised on every run/experiment
# you can just remove this section or certain callback if you don't want tu use it
# if you specify the same callback class also in experiment config then it will overwrite the one set here
# all of the callback parameters will be passed to it on initialization
default_callbacks:
    chekpointing:
        class: "pytorch_lightning.callbacks.ModelCheckpoint"
        args:
            monitor: "val_acc"      # name of the logged metric which determines when model is improving
            save_top_k: 1           # save k best lightning_models (determined by above metric)
            save_last: True         # additionaly always save model from last epoch
            mode: "max"             # can be "max" or "min"
    early_stopping:
        class: "pytorch_lightning.callbacks.EarlyStopping"
        args:
            monitor: "val_acc"      # name of the logged metric which determines when model is improving
            patience: 5             # how many epochs of not improving until training stops
            mode: "max"             # can be "max" or "min"


# use profiler to print execution time profiling after training ends
profiler:
    use_profiler: True
    class: "pytorch_lightning.profiler.SimpleProfiler"
    args: {}


# path to data folder (either absolute or relative to placement of 'train.py' file)
data_dir: "data/"


# path to logs folder (either absolute or relative to placement of 'train.py' file)
logs_dir: "logs/"
