# @package _global_

# to execute this experiment run:
# python train.py experiment=baseline

defaults:
  - override /datamodule: ebay.yaml
  - override /model: multi_label.yaml
  - override /callbacks: ema.yaml
  - override /logger: wandb.yaml
  - override /trainer: ddp.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
name: "multilabel_swinl_ema_poly_0.5_cosine_large"

seed: 42

trainer:
  min_epochs: 20
  max_epochs: 20
  gpus: 8

model:
  lr: 0.00015
  head_init_scale: 0.001
  weight_decay: 0.0001
  warmup_steps: 57190
  cosine_decay_steps: 228760
  net:
    name: swin_large_patch4_window12_384_in22k
    drop_path_rate: 0.5
  output_dim: 1536
  optimizer: adamw
  multi_label_smoothing: True
  poly_loss_weight: 0.5

datamodule:
  batch_size: 14
  multi_label: True
  train_transforms:
    - _target_: torchvision.transforms.RandomResizedCrop
      size: [384, 384]
      scale: [0.2, 1.0]
    - _target_: torchvision.transforms.RandomHorizontalFlip
    - _target_: torchvision.transforms.TrivialAugmentWide
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
    - _target_: torchvision.transforms.RandomErasing
      p: 0.25
  val_transforms:
    - _target_: torchvision.transforms.Resize
      size: [384, 384]
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

logger:
  wandb:
    tags: ["ebay", "${name}"]
