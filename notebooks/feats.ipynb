{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517594b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19490d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize([384, 384]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "        train_anno = pd.read_csv(\"../data/eBay/metadata/train.csv\").to_dict()\n",
    "        val_anno = pd.read_csv(\"../data/eBay/metadata/val.csv\").to_dict()\n",
    "        self.total_imgs = list(train_anno[\"IMAGE_PATH\"].values()) + list(val_anno[\"IMAGE_PATH\"].values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = \"../data/eBay/Images/\" + self.total_imgs[idx]\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad09e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = MyDataset()\n",
    "data_loader = DataLoader(\n",
    "    my_dataset, batch_size=32, shuffle=False, num_workers=4, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe33f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"convnext_base_384_in22ft1k\", pretrained=True)\n",
    "model.reset_classifier(-1)\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feats = []\n",
    "with torch.no_grad():\n",
    "    for image in tqdm(data_loader):\n",
    "        image = image.cuda()\n",
    "        feats = model(image)\n",
    "        image_feats.append(feats.cpu())\n",
    "image_feats = torch.cat(image_feats, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"convnext384_feats.npy\", image_feats.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "class RoBERTa(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"roberta-base\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        self.model = RobertaModel.from_pretrained(\"roberta-base\").eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_text):\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(self.model.device)\n",
    "        outputs = self.model(**inputs).last_hidden_state\n",
    "        masks = inputs[\"attention_mask\"]\n",
    "        outputs = outputs * masks.unsqueeze(2)\n",
    "        outputs = torch.sum(outputs, dim=1) / torch.sum(masks, dim=1, keepdim=True)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTextDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        train_anno = pd.read_csv(\"../data/eBay/metadata/train.csv\").to_dict()\n",
    "        val_anno = pd.read_csv(\"../data/eBay/metadata/val.csv\").to_dict()\n",
    "        self.total_texts = list(train_anno[\"AUCT_TITL\"].values()) + list(val_anno[\"AUCT_TITL\"].values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.total_texts[idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_textdataset = MyTextDataset()\n",
    "text_data_loader = DataLoader(\n",
    "    my_textdataset, batch_size=512, shuffle=False, num_workers=4, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = RoBERTa()\n",
    "roberta = roberta.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feats = []\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(text_data_loader):\n",
    "        feats = roberta(text)\n",
    "        text_feats.append(feats.cpu())\n",
    "text_feats = torch.cat(text_feats, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"roberta_avg_feats.npy\", text_feats.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize([224, 224]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "        index_anno = pd.read_csv(\"../data/eBay/metadata/index.csv\").to_dict()\n",
    "        self.total_imgs = list(index_anno[\"IMAGE_PATH\"].values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = \"../data/eBay/Images/\" + self.total_imgs[idx]\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = MyDataset()\n",
    "data_loader = DataLoader(\n",
    "    my_dataset, batch_size=64, shuffle=False, num_workers=4, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"convnext_base_in22k\", pretrained=True)\n",
    "# head = model.head.eval().cuda()\n",
    "# model.reset_classifier(-1)\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_feats = []\n",
    "image_labels = []\n",
    "with torch.no_grad():\n",
    "    for image in tqdm(data_loader):\n",
    "        image = image.cuda()\n",
    "#         feats_map = model.forward_features(image)\n",
    "#         feats = model.forward_head(feats_map)\n",
    "#         image_feats.append(feats.cpu())\n",
    "#         logits = head(feats_map)\n",
    "        logits = model(image)\n",
    "        labels = torch.argmax(logits, dim=1)\n",
    "        image_labels.append(labels.cpu())\n",
    "\n",
    "# image_feats = torch.cat(image_feats, dim=0)\n",
    "image_labels = torch.cat(image_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad689a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"query1_convnext384_feats.npy\", image_feats.numpy())\n",
    "np.save(\"index_in22k_labels.npy\", image_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(image_labels.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb16c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
