# Hackathon template
A convenient starting template for most deep learning projects. Built with <b>PyTorch Lightning</b> and <b>Weights&Biases</b>.
<br>


## Setup
Read [SETUP.md](SETUP.md)
<br><br>


## Project structure
The directory structure of new project looks like this: 
```
├── project
│   ├── data                    <- Data from third party sources
│   │
│   ├── data_modules            <- Data related modules
│   │   ├── datamodules.py              <- "LightningDataModule" modules
│   │   ├── datasets.py                 <- "Dataset" modules
│   │   └── transforms.py               <- Data transformations
│   │
│   ├── logs                    <- Logs generated by Weights&Biases and PyTorch Lightning
│   │
│   ├── utils                   <- Any extra scripts not belonging to training pipeline
│   │
│   ├── notebooks               <- Jupyter notebooks
│   │
│   ├── lightning_modules       <- PyTorch Lightning related modules
│   │   ├── callbacks.py                <- Training callbacks
│   │   └── init_utils.py               <- Some useful initializers
│   │
│   ├── models                  <- All your models should be located here
│   │   ├── simple_mnist_classifier     <- Example model
│   │   │   ├── lightning_module.py             <- Contains train/val/test step methods
│   │   │   └── models.py                       <- Model declarations used in lightning_module.py
│   │   │
│   │   ├── simple_cifar10_classifier   <- Example model
│   │   │   ├── lightning_module.py             <- Contains train/val/test step methods
│   │   │   └── models.py                       <- Model declarations used in lightning_module.py
│   │   │
│   │   └── ...
│   │
│   ├── project_config.yaml     <- Project configuration
│   ├── run_configs.yaml        <- Run configurations
│   ├── execute_sweep.py        <- Special file for executing wandb sweeps (hyperparameter search)
│   └── train.py                <- Train model
│
├── .gitignore
├── LICENSE
├── README.md
├── SETUP.md
├── TIPS.md
└── requirements.txt
```

## Project config parameters 
#### [project_config.yaml](project/project_config.yaml)
```yaml
num_of_gpus: -1

resume_training:
    lightning_ckpt:
        resume_from_ckpt: False
        ckpt_path: "logs/checkpoints/epoch=2.ckpt"
    wandb:
        resume_wandb_run: False
        wandb_run_id: "8uuomodb"

loggers:
    wandb:
        project: "hackathon_template_test"
        team: "kino"
        group: None
        job_type: "train"
        tags: []
        log_model: True
        offline: False

callbacks:
    checkpoint:
        monitor: "val_acc"
        save_top_k: 1
        save_last: True
        mode: "max"
    early_stop:
        monitor: "val_acc"
        patience: 100
        mode: "max"

printing:
    progress_bar_refresh_rate: 5
    weights_summary: "top"  # "full"
    profiler: False
```

## Run config parameters
#### [run_configs.yaml](project/run_configs.yaml)
You can store many run configurations in this file.<br>
Example run configuration:
```yaml
CIFAR10_CLASSIFIER_V2:
    trainer:
        max_epochs: 10
        gradient_clip_val: 0.5
        accumulate_grad_batches: 1
        limit_train_batches: 1.0
    model:
        name: "simple_cifar10_classifier"
        lr: 0.002
        weight_decay: 0.00001
        output_size: 10
        lin1_size: 512
        lin2_size: 256
    dataset:
        name: "CIFAR10DataModule"
        batch_size: 256
        train_val_split_ratio: 0.9
        num_workers: 1
        pin_memory: False
```

The run configuration that you want to train with needs to be chose in [train.py](project/train.py):
```python
if __name__ == "__main__":
    RUN_CONFIG = "MNIST_CLASSIFIER_V1"  # choose your run config here

    project_conf = load_config("project_config.yaml")
    run_conf = load_config("run_configs.yaml")[RUN_CONFIG]
    
    train(project_config=project_conf, run_config=run_conf)  # train with chosen run config
```


## Tips
For training tips read [TIPS.md](TIPS.md)
