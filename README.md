# Hackathon template
A convenient starting template for most deep learning projects. Built with <b>PyTorch Lightning</b> and <b>Weights&Biases</b>.
<br>


## Setup
Read [SETUP.md](SETUP.md) for more info and explanations or just do this quick setup:
```
git clone https://github.com/kinoai/hackathon-template
cd hackathon-template
conda update conda
conda env create -f conda_env.yaml -n hack_env
conda activate hack_env
pip install -r requirements.txt
wandb login
cd project
python train.py
```
<br>


## Project structure
The directory structure of new project looks like this: 
```
├── project
│   ├── data                    <- Data from third party sources
│   │
│   ├── logs                    <- Logs generated by Weights&Biases and PyTorch Lightning
│   │
│   ├── notebooks               <- Jupyter notebooks
│   │
│   ├── utils                   <- Different utilities
│   │   ├── callbacks.py            <- Useful training callbacks
│   │   ├── execute_sweep.py        <- Special file for executing Weights&Biases sweeps
│   │   ├── init_utils.py           <- Useful initializers
│   │   └── predict_example.py      <- Example of inference with trained model 
│   │
│   ├── data_modules            <- All your data modules should be located here!
│   │   ├── example_datamodule      <- Each datamodule should be located in separate folder!
│   │   │   ├── datamodule.py           <- Contains 'DataModule' class
│   │   │   ├── datasets.py             <- Contains pytorch 'Dataset' classes
│   │   │   └── transforms.py           <- Contains data transformations
│   │   ├── ...
│   │   └── ...
│   │
│   ├── models                  <- All your models should be located here!
│   │   ├── example_model           <- Each model should be located in separate folder!
│   │   │   ├── lightning_module.py     <- Contains 'LitModel' class with train/val/test step methods
│   │   │   └── models.py               <- Model architectures used by lightning_module.py
│   │   ├── ...
│   │   └── ...
│   │
│   ├── project_config.yaml     <- Project configuration
│   ├── run_configs.yaml        <- Configurations of different runs/experiments
│   └── train.py                <- Train model with chosen run configuration
│
├── .gitignore
├── LICENSE
├── README.md
├── SETUP.md
├── conda_env.yaml
└── requirements.txt
```


## Project config parameters ([project_config.yaml](project/project_config.yaml))
```yaml
num_of_gpus: -1             <- '-1' to use all gpus available, '0' to train on cpu

resume_training:
    lightning_ckpt:                                  
        resume_from_ckpt: False     <- set True if you want to resume                    
        ckpt_path: "epoch=7.ckpt"   <- path to your checkpoint
    wandb:
        resume_wandb_run: False     <- set True if you want to resume Weight&Biases run
        wandb_run_id: "8uuomodb"    <- id of Weight&Biases run you want to resume

loggers:
    wandb:
        project: "project_name"     <- wandb project name
        team: "kino"                <- wandb entity name
        group: None                 <- wandb group name
        job_type: "train"           <- wandb job_type name
        tags: []                    <- wandb tags name
        log_model: True             <- set True if you want to upload ckpts to wandb automatically
        offline: False              <- set True if you want to store all data locally

callbacks:
    checkpoint:
        monitor: "val_acc"      <- name of the logged metric that determines when model is improving
        save_top_k: 1           <- save k best models (determined by above metric)
        save_last: True         <- additionaly always save model from last epoch
        mode: "max"             <- can be "max" or "min"
    early_stop:
        monitor: "val_acc"      <- name of the logged metric that determines when model is improving
        patience: 100           <- how many epochs of not improving until training stops
        mode: "max"             <- can be "max" or "min"

printing:
    progress_bar_refresh_rate: 5    <- refresh rate of training bar in terminal
    weights_summary: "top"          <- print summary of model (alternatively "full")
    profiler: False                 <- set True if you want to see execution time profiling
```


## Run config parameters ([run_configs.yaml](project/run_configs.yaml))
You can store many run configurations in this file.<br>
Example run configuration:
```yaml
MNIST_CLASSIFIER_V1:
    trainer:                                            <- lightning 'Trainer' parameters (all except 'max_epochs' are optional)
        max_epochs: 5                                       
        gradient_clip_val: 0.5                              
        accumulate_grad_batches: 3                          
        limit_train_batches: 1.0                            
    model:                                              <- all of the parameters here will be passed to 'LitModel' in 'hparams' dictionary
        model_folder: "simple_mnist_classifier"             <- name of folder from which 'lightning_module.py' (with 'LitMdodel' class) will be loaded
        lr: 0.001                                           
        weight_decay: 0.000001                              
        input_size: 784                                     
        output_size: 10                                     
        lin1_size: 256                                      
        lin2_size: 256                                      
        lin3_size: 128                                      
    dataset:                                            <- all of the parameters here will be passed to 'DataModule' in 'hparams' dictionary
        datamodule_folder: "mnist_digits_datamodule"        <- name of folder from which 'datamodule.py' (with 'DataModule' class) will be loaded
        batch_size: 256                                     
        train_val_split_ratio: 0.9                          
        num_workers: 1                                      
        pin_memory: False                                   
```


## Workflow
1. Add your model to `project/models` folder<br>
    (you need to create folder with `lightning_module.py` file containing `LitModel` class)
2. Add your datamodule to `project/data_modules` folder<br>
    (you need to create folder with `datamodule.py` file containig `DataModule` class)
3. Add your run config to [run_configs.yaml](project/run_configs.yaml) (specify there folders containing your model and datamodule)
3. Configure [project_config.yaml](project/project_config.yaml)
4. Run training:<br>
    Either pass run config name as an argument:
    ```
    python train.py -c MNIST_CLASSIFIER_V1
    python train.py --conf_name MNIST_CLASSIFIER_V1
    ```
   Or modify default run config name in [train.py](project/train.py):
    ```python
    if __name__ == "__main__":
        parser = ArgumentParser()
        parser.add_argument("-c", "--conf_name", type=str, default="MNIST_CLASSIFIER_V1")
        args = parser.parse_args()
    
        main(run_config_name=args.conf_name)
    ```
