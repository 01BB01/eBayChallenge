# Hackathon template
A convenient starting template for most deep learning projects. Built with <b>PyTorch Lightning</b> and <b>Weights&Biases</b>.
<br>


## Setup
Read [SETUP.md](SETUP.md)
<br><br>


## Project structure
The directory structure of new project looks like this: 
```
├── project
│   ├── data                <- Data from third party sources
│   │
│   ├── logs                <- Logs generated by Weights&Biases and PyTorch Lightning
│   │
│   ├── docs                <- Useful pdf files
│   │
│   ├── hack_utils          <- Any extra scripts not belonging to training pipeline
│   │
│   ├── notebooks           <- Jupyter notebooks
│   │
│   ├── lightning_modules   <- PyTorch Lightning related modules
│   │   ├── data_modules                <- Data related modules
│   │   │   ├── datamodules.py              <- "LightningDataModule" modules
│   │   │   ├── datasets.py                 <- "Dataset" modules
│   │   │   └── transforms.py               <- Data transformations (data preprocessing)
│   │   │
│   │   ├── callbacks.py                <- Training callbacks
│   │   └── init_utils.py               <- Some useful initializers
│   │
│   ├── models
│   │   ├── simple_mnist_classifier     <- Example model
│   │   │   ├── lightning_module.py         <- Contains train/val/test step methods
│   │   │   ├── models.py                   <- Neural networks declarations
│   │   │   ├── predict.py                  <- Predicting from trained model
│   │   │   └── __init__.py
│   │   │
│   │   ├── simple_cifar10_classifier   <- Example model
│   │   │   ├── lightning_module.py
│   │   │   ├── models.py
│   │   │   ├── predict.py
│   │   │   └── __init__.py
│   │   │
│   │   └── ...
│   │
│   ├── config.yaml         <- Training configuration
│   ├── execute_sweep.py    <- Special file for executing wandb sweeps (hyperparameter search)
│   └── train.py            <- Train model
│
├── .gitignore
├── LICENSE
├── README.md
├── SETUP.md
├── TIPS.md
└── requirements.txt
```

## Config parameters 
#### [config.yaml](project/config.yaml):
```yaml
num_of_gpus: -1

model_configs:
    simple_mnist_classifier_v1:
        model_folder_name: "simple_mnist_classifier"
        max_epochs: 3
        lr: 0.001
        batch_size: 64
        weight_decay: 0.000001
        gradient_clip_val: 0.5
        accumulate_grad_batches: 1
        input_size: 784
        output_size: 10
        lin1_size: 256
        lin2_size: 256
        lin3_size: 128
        dataset:
            class_name: "MNISTDataModule"
            train_transforms: "minst_train_preprocess"
            test_transforms: "minst_test_preprocess"
            train_val_split_ratio: 0.8
            num_workers: 1
            pin_memory: False

    transfer_learning_cifar10_classifier_v1:
        ...

    transfer_learning_cifar10_classifier_v2:
        ...

resume_training:
    lightning_ckpt:
        resume_from_ckpt: False
        ckpt_path: "logs/checkpoints/epoch=2.ckpt"
    wandb:
        resume_wandb_run: False
        wandb_run_id: "8uuomodb"

loggers:
    wandb:
        project: "hackathon_template_test"
        team: "kino"
        group: None
        job_type: "train"
        tags: []
        log_model: True
        offline: False

callbacks:
    checkpoint:
        monitor: "val_acc"
        save_top_k: 1
        save_last: True
        mode: "max"
    early_stop:
        monitor: "val_acc"
        patience: 100
        mode: "max"

printing:
    progress_bar_refresh_rate: 5
    weights_summary: "top"
    profiler: False
```

## Tips
Read [TIPS.md](TIPS.md)
